{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 4: Under the Hood: Training a Digit Classifier\n"
      ],
      "id": "7c655664"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Pkg; Pkg.activate(\"book\")"
      ],
      "id": "dcdf4ed1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Packages\n",
        "using DataFrames\n",
        "using Flux\n",
        "using Images\n",
        "using Measures\n",
        "using MLDatasets\n",
        "using MLUtils\n",
        "using OneHotArrays\n",
        "using Plots\n",
        "import UnicodePlots"
      ],
      "id": "d1430f75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating Gradients\n",
        "\n",
        "### Using [`Flux.jl`](https://fluxml.ai/Flux.jl/stable/models/basics/)\n",
        "\n",
        "Taking gradients in `Flux.jl` is as simple as calling `gradient` on a function. For example, to take the gradient of `f(x) = x^2` at `x = 2`, we can do the following:\n"
      ],
      "id": "2d3889b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f(x) = x^2\n",
        "df(x) = gradient(f, x)[1]\n",
        "df(2)"
      ],
      "id": "427b34f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we implement and visualise gradient descent from scratch in Julia. \n"
      ],
      "id": "a52e6d2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| fig-cap: Gradient descent for different learning rates\n",
        "#| label: fig-gd\n",
        "\n",
        "xmax = 10\n",
        "n = 100\n",
        "plt = plot(\n",
        "    range(-xmax, xmax, length=n), f;\n",
        "    label=\"f(x)\", lw=5, xlim=1.5 .* [-xmax, xmax], \n",
        "    xlab=\"Parameter\", ylab=\"Loss\",legend=false\n",
        ")\n",
        "\n",
        "nsteps = 10\n",
        "lrs = [0.05, 0.3, 0.975, 1.025]\n",
        "descend(x;lr=0.1) = x - lr * df(x)\n",
        "x = [-0.75xmax]       \n",
        "x = repeat(x,length(lrs),1)                             # repeat x for each learning rate\n",
        "plts = [deepcopy(plt) for i in 1:length(lrs)]           # repeat plt for each learning rate\n",
        "anim = @animate for j in 1:nsteps\n",
        "    global x = hcat(x, zeros(size(x,1)))                # add column of zeros to x\n",
        "    for (i, lr) in enumerate(lrs)\n",
        "        _plt = plot(plts[i], title=\"lr = $lr\", ylims=(0,f(xmax)), legend=false)\n",
        "        scatter!([x[i,j]], [f(x[i,j])]; label=nothing, ms=5, c=:red)    # plot current point\n",
        "        x[i,j+1] = descend(x[i,j]; lr=lr)                               # descend\n",
        "        Δx = x[i,j+1] - x[i,j]\n",
        "        Δy = f(x[i,j+1]) - f(x[i,j])\n",
        "        quiver!([x[i,j]],[f(x[i,j])],quiver=([Δx],[0]),c=:red)          # horizontal arrow\n",
        "        quiver!([x[i,j+1]],[f(x[i,j])],quiver=([0],[Δy]),c=:red)        # vertical arrow\n",
        "        plts[i] = _plt\n",
        "    end\n",
        "    plot(\n",
        "        plts..., layout=(1,length(plts)), \n",
        "        size=(length(plts)*300,300), legend=false,\n",
        "        plot_title=\"Step $j\", margin = 5mm\n",
        "    )\n",
        "end\n",
        "gif(anim, fps=0.5)"
      ],
      "id": "fig-gd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Digit Classifier\n",
        "\n",
        "The MNIST dataset can be loaded in Julia as follows:\n"
      ],
      "id": "d698bad8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data\n",
        "X, y = MLDatasets.MNIST(:train)[:]\n",
        "y_enc = Flux.onehotbatch(y, 0:9)\n",
        "Xtest, ytest = MLDatasets.MNIST(:test)[:]\n",
        "ytest_enc = onehotbatch(ytest, 0:9)\n",
        "mosaic(map(i -> convert2image(MNIST, X[:,:,i]), rand(1:60000,100)), ncol=10)"
      ],
      "id": "162858b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can preprocess the data as follows:\n"
      ],
      "id": "8ad1f954"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "i_train, i_val = [], []\n",
        "for (k,v) in group_indices(y)\n",
        "    _i_train, _i_val = splitobs(v, at=0.7)\n",
        "    push!(i_train, _i_train...)\n",
        "    push!(i_val, _i_val...)\n",
        "end\n",
        "Xtrain, ytrain = X[:,:,i_train], y_enc[:,i_train]\n",
        "Xval, yval = X[:,:,i_val], y_enc[:,i_val]"
      ],
      "id": "8859e80b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define a data loader:\n"
      ],
      "id": "15855321"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "batchsize = 128\n",
        "train_set = DataLoader((Xtrain, ytrain), batchsize=batchsize, shuffle=true)\n",
        "val_set = DataLoader((Xval, yval), batchsize=batchsize)"
      ],
      "id": "dd345edb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now define a model, based on how we preprocessed the data:\n"
      ],
      "id": "1a3cfa7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Chain(\n",
        "    Flux.flatten,\n",
        "    Dense(28^2, 32, relu),\n",
        "    Dense(32, 10),\n",
        "    softmax\n",
        ")"
      ],
      "id": "bb0c5424",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, what's left to do is to define a loss function and an optimiser:\n"
      ],
      "id": "a758bf9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss(y_hat, y) = Flux.Losses.crossentropy(y_hat, y)\n",
        "opt_state = Flux.setup(Adam(),model)"
      ],
      "id": "37cab200",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start training, we define some helper functions:\n"
      ],
      "id": "09274fc9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Callbacks:\n",
        "function accuracy(model, data::DataLoader)\n",
        "    acc = 0\n",
        "    for (x,y) in data\n",
        "        acc += sum(onecold(model(x)) .== onecold(y)) / size(y,2)\n",
        "    end\n",
        "    return acc / length(data)\n",
        "end\n",
        "\n",
        "function avg_loss(model, data::DataLoader)\n",
        "    _loss = 0\n",
        "    for (x,y) in data\n",
        "        _loss += loss(model(x), y)[1]\n",
        "    end\n",
        "    return _loss / length(data)\n",
        "end"
      ],
      "id": "07e6b4d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As very last step, we set up our traning logs:\n"
      ],
      "id": "9cc3419f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final setup:\n",
        "nepochs = 100\n",
        "log = []\n",
        "acc_train, acc_val = accuracy(model, train_set), accuracy(model, val_set)\n",
        "loss_train, loss_val = avg_loss(model, train_set), avg_loss(model, val_set)\n",
        "results = Dict(\n",
        "    :epoch => 0,\n",
        "    :acc_train => acc_train,\n",
        "    :acc_val => acc_val,\n",
        "    :loss_train => loss_train,\n",
        "    :loss_val => loss_val\n",
        ")\n",
        "push!(log, results)"
      ],
      "id": "dcf997aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we finally train our model:\n"
      ],
      "id": "1fd86071"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training loop:\n",
        "for epoch in 1:nepochs\n",
        "\n",
        "    for (i, data) in enumerate(train_set)\n",
        "\n",
        "        # Extract data:\n",
        "        input, label = data\n",
        "\n",
        "        # Compute loss and gradient:\n",
        "        val, grads = Flux.withgradient(model) do m\n",
        "            result = m(input)\n",
        "            loss(result, label)\n",
        "        end\n",
        "\n",
        "        # Detect loss of Inf or NaN. Print a warning, and then skip update!\n",
        "        if !isfinite(val)\n",
        "            @warn \"loss is $val on item $i\" epoch\n",
        "            continue\n",
        "        end\n",
        "\n",
        "        Flux.update!(opt_state, model, grads[1])\n",
        "\n",
        "    end\n",
        "\n",
        "    # Monitor progress:\n",
        "    acc_train, acc_val = accuracy(model, train_set), accuracy(model, val_set)\n",
        "    loss_train, loss_val = avg_loss(model, train_set), avg_loss(model, val_set)\n",
        "    results = Dict(\n",
        "        :epoch => epoch,\n",
        "        :acc_train => acc_train,\n",
        "        :acc_val => acc_val,\n",
        "        :loss_train => loss_train,\n",
        "        :loss_val => loss_val\n",
        "    )\n",
        "    push!(log, results)\n",
        "\n",
        "    # Print progress:\n",
        "    results_df = DataFrame(log)\n",
        "    vals = Matrix(results_df[2:end,[:loss_train,:loss_val]])\n",
        "    plt = UnicodePlots.lineplot(1:epoch, vals; \n",
        "        name=[\"Train\",\"Validation\"], title=\"Loss in epoch $epoch\", xlim=(1,nepochs))\n",
        "    UnicodePlots.display(plt)\n",
        "\n",
        "end"
      ],
      "id": "b7b21426",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@fig-mnist shows the training and validation loss and accuracy over epochs. Evidently, the model is overfitting, as the validation loss increases after bottoming out at around epoch 20.\n"
      ],
      "id": "2d8a0c8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "#| fig-cap: Training and validation loss and accuracy\n",
        "#| label: fig-mnist\n",
        "\n",
        "output = DataFrame(log)\n",
        "output = output[2:end,:]\n",
        "\n",
        "anim = @animate for epoch in 1:maximum(output.epoch)\n",
        "    p_loss = plot(output[1:epoch,:epoch], Matrix(output[1:epoch,[:loss_train,:loss_val]]), \n",
        "        label=[\"Train\" \"Validation\"], title=\"Loss\", legend=:topleft)\n",
        "    p_acc = plot(output[1:epoch,:epoch], Matrix(output[1:epoch,[:acc_train,:acc_val]]), \n",
        "        label=[\"Train\" \"Validation\"], title=\"Accuracy\", legend=:topleft)\n",
        "    plot(p_loss, p_acc, layout=(1,2), size=(800,400))\n",
        "end\n",
        "gif(anim, fps=5)"
      ],
      "id": "fig-mnist",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.6",
      "language": "julia",
      "display_name": "Julia 1.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}