[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "book",
    "section": "",
    "text": "Preface\nThis book provides an overview of a pure Julia implementation of the fastai book: Deep Learning for Coders with fastai and PyTorch. The original book works with Python and PyTorch. This book works with Julia and relies primarily on Flux.jl. The original book is written by Jeremy Howard and Sylvain Gugger. This book is written by participants of the Julia for Deep Learning course."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "chapters/c4.html#calculating-gradients",
    "href": "chapters/c4.html#calculating-gradients",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.1 Calculating Gradients",
    "text": "2.1 Calculating Gradients\n\n2.1.1 Using Flux.jl\nTaking gradients in Flux.jl is as simple as calling gradient on a function. For example, to take the gradient of f(x) = x^2 at x = 2, we can do the following:\n\nf(x) = x^2\ndf(x) = gradient(f, x)[1]\ndf(2)\n\n4.0\n\n\nBelow we implement and visualise gradient descent from scratch in Julia.\n\nxmax = 10\nn = 100\nplt = plot(\n    range(-xmax, xmax, length=n), f;\n    label=\"f(x)\", lw=5, xlim=1.5 .* [-xmax, xmax], \n    xlab=\"Parameter\", ylab=\"Loss\",legend=false\n)\n\nnsteps = 10\nlrs = [0.05, 0.3, 0.975, 1.025]\ndescend(x;lr=0.1) = x - lr * df(x)\nx = [-0.75xmax]       \nx = repeat(x,length(lrs),1)                             # repeat x for each learning rate\nplts = [deepcopy(plt) for i in 1:length(lrs)]           # repeat plt for each learning rate\nanim = @animate for j in 1:nsteps\n    global x = hcat(x, zeros(size(x,1)))                # add column of zeros to x\n    for (i, lr) in enumerate(lrs)\n        _plt = plot(plts[i], title=\"lr = $lr\", ylims=(0,f(xmax)), legend=false)\n        scatter!([x[i,j]], [f(x[i,j])]; label=nothing, ms=5, c=:red)    # plot current point\n        x[i,j+1] = descend(x[i,j]; lr=lr)                               # descend\n        Δx = x[i,j+1] - x[i,j]\n        Δy = f(x[i,j+1]) - f(x[i,j])\n        quiver!([x[i,j]],[f(x[i,j])],quiver=([Δx],[0]),c=:red)          # horizontal arrow\n        quiver!([x[i,j+1]],[f(x[i,j])],quiver=([0],[Δy]),c=:red)        # vertical arrow\n        plts[i] = _plt\n    end\n    plot(\n        plts..., legend=false,\n        plot_title=\"Step $j\", margin = 5mm\n    )\nend\ngif(anim, joinpath(www_path, \"c4_gd.gif\"), fps=0.5)\n\n[ Info: Saved animation to /Users/patrickaltmeyer/code/julia-deeplearning/book/www/c4_gd.gif\n\n\n\n\nFigure 2.1: Gradient descent for different learning rates"
  },
  {
    "objectID": "chapters/c4.html#training-a-digit-classifier",
    "href": "chapters/c4.html#training-a-digit-classifier",
    "title": "2  Chapter 4: Under the Hood: Training a Digit Classifier",
    "section": "2.2 Training a Digit Classifier",
    "text": "2.2 Training a Digit Classifier\nThe MNIST dataset can be loaded in Julia as follows:\n\n# Data\nX, y = MLDatasets.MNIST(:train)[:]\ny_enc = Flux.onehotbatch(y, 0:9)\nXtest, ytest = MLDatasets.MNIST(:test)[:]\nytest_enc = onehotbatch(ytest, 0:9)\nmosaic(map(i -&gt; convert2image(MNIST, X[:,:,i]), rand(1:60000,100)), ncol=10)\n\n\n\n\nWe can preprocess the data as follows:\n\ni_train, i_val = [], []\nfor (k,v) in group_indices(y)\n    _i_train, _i_val = splitobs(v, at=0.7)\n    push!(i_train, _i_train...)\n    push!(i_val, _i_val...)\nend\nXtrain, ytrain = X[:,:,i_train], y_enc[:,i_train]\nXval, yval = X[:,:,i_val], y_enc[:,i_val]\n\n([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Bool[0 0 … 0 0; 0 0 … 1 1; … ; 0 0 … 0 0; 0 0 … 0 0])\n\n\nNext, we define a data loader:\n\nbatchsize = 128\ntrain_set = DataLoader((Xtrain, ytrain), batchsize=batchsize, shuffle=true)\nval_set = DataLoader((Xval, yval), batchsize=batchsize)\n\n141-element DataLoader(::Tuple{Array{Float32, 3}, OneHotMatrix{UInt32, Vector{UInt32}}}, batchsize=128)\n  with first element:\n  (28×28×128 Array{Float32, 3}, 10×128 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)\n\n\nWe can now define a model, based on how we preprocessed the data:\n\nmodel = Chain(\n    Flux.flatten,\n    Dense(28^2, 32, relu),\n    Dense(32, 10),\n    softmax\n)\n\n\nChain(\n  Flux.flatten,\n  Dense(784 =&gt; 32, relu),               # 25_120 parameters\n  Dense(32 =&gt; 10),                      # 330 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 25_450 parameters, 99.664 KiB.\n\n\n\nFinally, what’s left to do is to define a loss function and an optimiser:\n\nloss(y_hat, y) = Flux.Losses.crossentropy(y_hat, y)\nopt_state = Flux.setup(Adam(),model)\n\n\n(layers = ((), (weight = Leaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), (Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), (Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))), σ = ()), (weight = Leaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), (Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))), bias = Leaf(Adam{Float64}(0.001, (0.9, 0.999), 1.0e-8), (Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))), σ = ()), ()),)\n\n\n\nBefore we start training, we define some helper functions:\n\n# Callbacks:\nfunction accuracy(model, data::DataLoader)\n    acc = 0\n    for (x,y) in data\n        acc += sum(onecold(model(x)) .== onecold(y)) / size(y,2)\n    end\n    return acc / length(data)\nend\n\nfunction avg_loss(model, data::DataLoader)\n    _loss = 0\n    for (x,y) in data\n        _loss += loss(model(x), y)[1]\n    end\n    return _loss / length(data)\nend\n\navg_loss (generic function with 1 method)\n\n\nAs a very last step, we set up our training logs:\n\n# Final setup:\nnepochs = 100\nlog = []\nacc_train, acc_val = accuracy(model, train_set), accuracy(model, val_set)\nloss_train, loss_val = avg_loss(model, train_set), avg_loss(model, val_set)\nresults = Dict(\n    :epoch =&gt; 0,\n    :acc_train =&gt; acc_train,\n    :acc_val =&gt; acc_val,\n    :loss_train =&gt; loss_train,\n    :loss_val =&gt; loss_val\n)\npush!(log, results)\n\n1-element Vector{Any}:\n Dict{Symbol, Real}(:acc_train =&gt; 0.1270086492043626, :acc_val =&gt; 0.13445860265733012, :loss_train =&gt; 2.3232887f0, :loss_val =&gt; 2.313282f0, :epoch =&gt; 0)\n\n\nBelow we finally train our model:\n\n# Training loop:\nfor epoch in 1:nepochs\n\n    for (i, data) in enumerate(train_set)\n\n        # Extract data:\n        input, label = data\n\n        # Compute loss and gradient:\n        val, grads = Flux.withgradient(model) do m\n            result = m(input)\n            loss(result, label)\n        end\n\n        # Detect loss of Inf or NaN. Print a warning, and then skip update!\n        if !isfinite(val)\n            @warn \"loss is $val on item $i\" epoch\n            continue\n        end\n\n        Flux.update!(opt_state, model, grads[1])\n\n    end\n\n    # Monitor progress:\n    acc_train, acc_val = accuracy(model, train_set), accuracy(model, val_set)\n    loss_train, loss_val = avg_loss(model, train_set), avg_loss(model, val_set)\n    results = Dict(\n        :epoch =&gt; epoch,\n        :acc_train =&gt; acc_train,\n        :acc_val =&gt; acc_val,\n        :loss_train =&gt; loss_train,\n        :loss_val =&gt; loss_val\n    )\n    push!(log, results)\n\n    # Print progress:\n    results_df = DataFrame(log)\n    vals = Matrix(results_df[2:end,[:loss_train,:loss_val]])\n    plt = UnicodePlots.lineplot(1:epoch, vals; \n        name=[\"Train\",\"Validation\"], title=\"Loss in epoch $epoch\", xlim=(1,nepochs))\n    UnicodePlots.display(plt)\n\nend\n\nFigure 2.2 shows the training and validation loss and accuracy over epochs. Evidently, the model is overfitting, as the validation loss increases after bottoming out at around epoch 20.\n\noutput = DataFrame(log)\noutput = output[2:end,:]\n\nanim = @animate for epoch in 1:maximum(output.epoch)\n    p_loss = plot(output[1:epoch,:epoch], Matrix(output[1:epoch,[:loss_train,:loss_val]]), \n        label=[\"Train\" \"Validation\"], title=\"Loss\", legend=:topleft)\n    p_acc = plot(output[1:epoch,:epoch], Matrix(output[1:epoch,[:acc_train,:acc_val]]), \n        label=[\"Train\" \"Validation\"], title=\"Accuracy\", legend=:topleft)\n    plot(p_loss, p_acc, layout=(1,2), size=(800,400))\nend\ngif(anim, joinpath(www_path, \"c4_mnist.gif\"), fps=5)\n\n[ Info: Saved animation to /Users/patrickaltmeyer/code/julia-deeplearning/book/www/c4_mnist.gif\n\n\n\n\nFigure 2.2: Training and validation loss and accuracy"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]